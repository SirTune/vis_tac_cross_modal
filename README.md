# “Touching to See” and “Seeing to Feel” using Generative Adversarial Networks (GANs)

Implementation of a Conditional Generative Adversarial Network for learning the cross-modal interaction between a visual and tactile persepective. This adapts images from the [ViTac Dataset](https://arxiv.org/pdf/1802.07490.pdf) consisting of fabric materials captured from a camera as a visual perspectiva and a GelSight sensor as the tactile perspective. we propose a novel framework for the cross-modal sensory data generation for visual and tactile perception. Taking texture perception as an example, we apply conditional generative adversarial networks to generate pseudo visual images or tactile outputs from data of the other modality.

![](https://github.com/SirTune/vis_tac_cross_modal/blob/master/img/cloth_images.png)

Extensive experiments on the ViTac dataset of cloth textures show that the proposed method can produce realistic outputs from other sensory inputs.
___
### CGAN

___
### Evaluation


___
### Classification
